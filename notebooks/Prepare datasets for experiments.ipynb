{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import fastText\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweet_recommendations.embeddings import word2vec, fast_text\n",
    "from tweet_recommendations.other_methods.method import Method\n",
    "from tweet_recommendations.data_processing.split_train_test import split_to_train_test_by_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_udi_subsampled_by_users(udi_dataset, fraction=0.1, verbose=False):\n",
    "    #udi_dataset = pd.read_csv(\"./data/udi_processed/udi_stemmed.csv\")\n",
    "    files = os.listdir(\"./data/udi_dataset/UDI-TwitterCrawl-Aug2012-Tweets-Parsed/\")\n",
    "    np.random.shuffle(files)\n",
    "    files = files[:int(fraction * len(files))]\n",
    "    usernames = []\n",
    "    for file in tqdm.tqdm_notebook(files, disable=not verbose, \n",
    "                                   desc=\"Loading user ids\"):\n",
    "            df = pd.read_csv(\"./data/udi_dataset/UDI-TwitterCrawl-Aug2012-Tweets-Parsed/\" + file, usecols=[\"ID\"])\n",
    "            df[\"username\"] = file.split(\".\")[0]\n",
    "            usernames.append(df)\n",
    "\n",
    "    usernames = pd.concat(usernames, ignore_index=True)\n",
    "    if verbose: print(\"Loaded tweets authors usernames\")\n",
    "    udi_dataset = udi_dataset.merge(usernames, on=\"ID\")\n",
    "    if verbose: print(\"Merged tweets authors usernames\")\n",
    "    return udi_dataset\n",
    "    \n",
    "    \n",
    "def unify_columns_in_udi_dataset(udi_dataset, verbose=False):\n",
    "    #udi_dataset = pd.read_pickle(\"./data/udi_dataset/temp_stemmed_dataset_with_users.pkl\")\n",
    "\n",
    "    columns_to_leave = [\"ID\", \"username\", \"RetCount\", \n",
    "                        \"Origin\", \"Text\", \"Hashtags\"]\n",
    "    columns_to_drop = list(set(udi_dataset.columns) - set(columns_to_leave))\n",
    "    udi_dataset.drop(columns=columns_to_drop, inplace=True)\n",
    "    udi_dataset.rename(columns={\"ID\": \"id\", \n",
    "                                \"username\": \"username\",\n",
    "                                \"RetCount\": \"retweet_count\",\n",
    "                                \"Origin\": \"text\", \n",
    "                                \"Text\": \"lemmas\",\n",
    "                                \"Hashtags\": \"hashtags\"}, \n",
    "                       inplace=True)\n",
    "    if verbose: print(\"Filtered columns\")    \n",
    "    udi_dataset.drop_duplicates([\"id\"], inplace=True)\n",
    "    if verbose: print(\"Dropped duplicates\")\n",
    "    \n",
    "    if verbose: print(\"Removing punctuation and numbers...\")\n",
    "    def remove_punctuation(lemmas):\n",
    "        result = []\n",
    "        if isinstance(lemmas, list):\n",
    "            for lemma in lemmas:\n",
    "                stripped = lemma.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "                if stripped and not stripped.isdigit():\n",
    "                    result.append(stripped)\n",
    "        return result\n",
    "    udi_dataset[\"lemmas\"] = udi_dataset[\"lemmas\"].str.split().apply(remove_punctuation)\n",
    "    if verbose: print(\"Converting hashtags...\")\n",
    "    udi_dataset[\"hashtags\"] = udi_dataset[\"hashtags\"].apply(lambda x: [{\"text\": h} for h in eval(x)])\n",
    "    return udi_dataset\n",
    "\n",
    "\n",
    "def load_udi_dataset(fraction=0.1, verbose=False):\n",
    "    udi_dataset = pd.read_csv(\"./data/udi_processed/udi_stemmed.csv\")\n",
    "    udi_dataset = load_udi_subsampled_by_users(udi_dataset, fraction, verbose)\n",
    "    udi_dataset = unify_columns_in_udi_dataset(udi_dataset, verbose)\n",
    "    udi_dataset = udi_dataset[udi_dataset[\"lemmas\"].str.len() > 1]\n",
    "    udi_dataset = udi_dataset[udi_dataset[\"hashtags\"].str.len() > 0]\n",
    "    return udi_dataset\n",
    "\n",
    "\n",
    "def load_our_dataset():\n",
    "    tweets_with_lemmas = pd.read_pickle(\"data/source_data/original_tweets_with_lemmas.p\")\n",
    "    our_dataset = tweets_with_lemmas[[\"id\", \"username\", \"retweet_count\", \"text\", \"lemmas\", \"hashtags\"]]\n",
    "    return our_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(lemmas_list, fasttext_model):\n",
    "    return np.stack([fasttext_model.get_word_vector(word) for word in lemmas_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(source_dataset, fasttext_model, verbose=False):\n",
    "    dataset = source_dataset[source_dataset[\"hashtags\"].str.len()>0]\n",
    "    dataset = dataset[dataset[\"lemmas\"].str.len()>0]\n",
    "    if verbose: print(\"Dropped empty tweets\")\n",
    "    if verbose: print(\"Filtering out hashtag below minimal frequency...\")\n",
    "    dataset = Method.drop_tweets_with_hashtags_that_occurred_less_than(dataset, minimal_hashtag_occurrence=3)\n",
    "    if verbose: print(\"Getting word embeddings...\")\n",
    "    dataset[\"word_embeddings\"] = dataset[\"lemmas\"].apply(partial(get_word_vectors, fasttext_model=fasttext_model))\n",
    "    if verbose:print(\"Calculating tweet embeddings...\")\n",
    "    dataset[\"embedding\"] = dataset[\"word_embeddings\"].apply(lambda x: x.mean(axis=0))\n",
    "    if verbose: print(\"Splitting to train & test\")\n",
    "    train_dataset, test_dataset = split_to_train_test_by_user(dataset)\n",
    "    if verbose: print(\"Done!\")\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b93b2adfc8e48679d66c07addd7ba04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading user ids', max=6693, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded tweets authors usernames\n",
      "Merged tweets authors usernames\n",
      "Filtered columns\n",
      "Dropped duplicates\n",
      "Removing punctuation and numbers...\n",
      "Converting hashtags...\n",
      "Dropped empty tweets\n",
      "Filtering out hashtag below minimal frequency...\n",
      "Getting word embeddings...\n",
      "Calculating tweet embeddings...\n",
      "Splitting to train & test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/SAMSUNG/projects/hashtag-recommendation-project/tweet_recommendations/data_processing/split_train_test.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  duplicates='drop').cat.codes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "288223 120628\n"
     ]
    }
   ],
   "source": [
    "udi_dataset = load_udi_dataset(fraction=0.05, verbose=True)\n",
    "eng_fasttext = fastText.load_model(\"/mnt/SAMSUNG/models/fasttext/english/cc.en.300.bin\")\n",
    "udi_train, udi_test = process_dataset(udi_dataset, eng_fasttext, verbose=True)\n",
    "print(len(udi_train), len(udi_test))\n",
    "del eng_fasttext\n",
    "\n",
    "udi_train.to_pickle(\"./data/experiments_datasets/udi_train.pkl\")\n",
    "udi_test.to_pickle(\"./data/experiments_datasets/udi_test.pkl\")\n",
    "\n",
    "del udi_dataset, udi_train, udi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped empty tweets\n",
      "Filtering out hashtag below minimal frequency...\n",
      "Getting word embeddings...\n",
      "Calculating tweet embeddings...\n",
      "Splitting to train & test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/SAMSUNG/projects/hashtag-recommendation-project/tweet_recommendations/data_processing/split_train_test.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  duplicates='drop').cat.codes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "36699 11054\n"
     ]
    }
   ],
   "source": [
    "pol_fasttext = fastText.load_model(\"/mnt/SAMSUNG/models/fasttext/polish/kgr10.plain.lemma.lower.skipgram.dim300.neg10.bin\")\n",
    "our_train, our_test = process_dataset(load_our_dataset(), pol_fasttext, verbose=True)\n",
    "print(len(our_train), len(our_test))\n",
    "del pol_fasttext\n",
    "our_train.to_pickle(\"./data/experiments_datasets/our_train.pkl\")\n",
    "our_test.to_pickle(\"./data/experiments_datasets/our_test.pkl\")\n",
    "\n",
    "del our_train, our_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
